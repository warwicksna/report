\section{Data Acquisition}

As mentioned in the design, we opted to authenticate our requests with OAuth to take advantage of the associated loosening of rate limits. Using OAuth entails creating a digital signature for every request so that Twitter's servers can verify that the request hasn't been tampered with while in flight. From the client's perspective it provides no assurance that the response from the server hasn't been altered. As such, I suspect that it is more about ensuring that developers take responsibility for the requests their applications make, for fear of having their access tokens revoked. Twitter holds its own web interface a lower standard of security; it doesn't use HTTPS by default.

Neither Perl nor Python have a core library for using OAuth, and after a few hours attempting to install a third-party library on joshua.dcs.warwick.ac.uk I decided to implement the OAuth signing procedure myself, in the interest of keeping the code portable. This ruled out Perl, since it does not include the HMAC-SHA1 algorithm in the core modules. Python, however, did so we now have the `api' function which given a method and some arguments will sign them. The application and user account information used for the signing are both hard-coded but would be trivial to alter; see the user guide for instructions.

Upon running this script, it quickly became evident that the Twitter servers were prone to throwing an eclectic range of errors including codes `500 Internal Server Error', `502 Bad Gateway' and ‘503 Service Unavailable'. Adding an exponential backoff to deal with these errors as they occurred helped, but as some of them only happened once every few days they caused serious data loss. As such, the script was adjusted to regularly store its internal state alongside the data it harvests, and thus be able to resume after errors with minimal data loss.

At one point the server returned corrupted data for a user's tweets without throwing an error. As tweets are placed into the database verbatim, this didn't trigger any errors and caused trouble later when the data was processed. The script now validates that all responses are valid JSON, which should mitigate any recurrences of this particular issue. Of course, if a corrupted response happens to be valid JSON it may pass this check unhindered.

As expected, the script mapped around 20 users per hour. As the number of fully-mapped nodes approached 3000, file I/O errors began appearing on the client side. Investigation suggested that this was an issue with Joshua, rather than SQLite:

\begin{quote}
The theoretical maximum number of rows in a table is 264 (18446744073709551616 or about 1.8e+19). This limit is unreachable since the maximum database size of 14 terabytes will be reached first.
\end{quote}
\cite{sqliteLimits}

Implementation of the conversion scripts went smoothly, though the initial attempts didn't scale well. This was fixed by using native data structures in lieu of repeated JSON parsing, and writing the output to disc in a streaming manner. The final scripts are as follows:
\begin{itemize}
\item generateGraphml.py generates a directed graph in graphml format, using user's ‘following' as outgoing edges and ‘followers' as incoming edges.
\item generateGraphmlLimited.py is an improved version of generateGraphml that ignores leaf nodes, and scales much better.
\item generateGraphmlTweets.py is based on generateGraphmlLimited but also considers directed tweets and retweets as outgoing edges.
\item tweetsToSQL.py converts tweets into the SQL format suitable to be played back over time in the previous year's interface.
\end{itemize}

It was observed that in practise the Twitter API's rate limit is 350 requests per hour per account per IP, so the tool can be used to harvest information on multiple different groups simultaneously without altering the user/application id, just by running it on computers with distinct external IP addresses.

\subsection{Reflection}
One observed issue was that all metrics of importance are biased towards the root node. This is an artifact of the breadth-first search (BFS) algorithm, and as such is hard to deal with. Removing the root node entirely from the final graph may help fix the obvious metrics, but the graph will still be distorted. Mapping the entirety of Twitter would fix this issue but is infeasible give the rate-limits, and using hashtags as a pure alternative to the BFS would have produced inferior results.

The other major issue - the failure to accrue more than 3000 fully mapped nodes on a particular target - was ultimately a consequence of the extremely late start (halfway through term 2). It takes around a week of uninterrupted harvesting to get that much data, so the problem didn\'t become apparent until late on, by which point we were committed to using SQLite and had insufficient time to re-write the code to use MySQL or request a different server. It was fortuatous that 3000 nodes were sufficient to demonstrate the lack of scalability of the old visualiser.

A more permanent approach to harvesting information from Twitter would be to obtain access to the streaming API, and construct code for fault tolerance from the ground up; performing extensive validation of all data received and verifying that it is stored correctly. A heavyweight alternative to SQLite should be used provided that it can be hosted locally; introducing a second network connection would needlessly introduce further possibilities for failure. After extensive testing with fault-ijection this could then be left to run for several months, and obtain a huge quantity of data. A possible extension would be to modify the algorithm to use a hybrid of the breadth-first-search and hashtag approaches.

Though the graphml generation scripts scale linearly, they are hindered by writing to disk being slow, and graphml being a verbose format. Thus the two ways to improve their speed would be to use the recipient program\'s API to construct the graph rather than writing it to disk, and using a more concise format than graphml. However, both of these would severely limit their compatibility. A simple extension would be to assign variable weights to edges based on the number of tweets and follows reinforcing them.

\subsection{Stanford Large Network Dataset Collection}
As explained previously in Section \ref{sec:des_snap}, the majority of the datasets available from the Stanford Large Network Dataset Collection are of the form $SourceNodeId$ $\rightarrow$ $TargetNodeId$ which represents edges present in the network.

For use with algorithms implemented with Giraph, these datasets need to processed into the JSON format shown in Listing \ref{lst:json}. The data available from SNAP is sorted by the $SourceNodeId$, and then by the $TargetNodeId$, which allows a simple parser to be constructed which processes the data line-by-line, and consequently node-by-node and edge-by-edge of each node.

\lstset{language=Java,caption={SNAP Parser},label=lst:snap,tabsize=2,breaklines=true,breakatwhitespace=true,frame=single}
\begin{lstlisting}[float]
while ((line = in.readLine()) != null) {
	node = line.split("\t");
	if (s.length() == 0) {
		current = node[0];
		s.append("["+current+",0,[");
	}
	if (node[0].equals(current)) {
		s.append("["+node[1]+",1],");
	} else {
		System.out.println(
			s.deleteCharAt(s.length()-1).toString()+"]]");
		s = new StringBuilder();
		current = node[0];
		s.append("["+current+",0,[");
		s.append("["+node[1]+",1],");
	}
}
\end{lstlisting}

Listing \ref{lst:snap} represents the core of the parser. Using a \verb/StringBuilder/, the parser continues appending edges whilst the source node remains the same. When the source node differs, the string being constructed is finished with closing brackets and printed to the standard output, and the process then repeats until all nodes and edges have been processed.