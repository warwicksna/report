\section{Data Acquisition}

\subsection{Twitter}
Our requirements were as follows:
\begin{itemize}
\item Sufficient data per item to construct a graph
\item Temporal data for use in last year's interface
\item Sufficient nodes to illustrate the cluster’s usefulness
\item Modern, real-world data
\end{itemize}

The social networking site Twitter met all these requirements. As an added bonus, it had information on two groups our customer has an interest in; the English Defence League (EDL) and Unite Against Fascism (UAF). Twitter is easily mapped onto a directed graph as it uses an asymmetric following system where user X can follow user Y without user Y having to follow user X. In addition to collecting the following/followers lists for each user we store the messages they broadcast, their `tweets'. Tweets may be addressed to particular users, and can thus be modelled as edges themselves. In addition to normal, hand-typed tweets there are  `retweets' where the user has essentially endorsed another user’s tweet by repeating it themselves. These can likewise be modelled as directed edges. Along with the text content of a tweet comes a significant amount of metadata such as its timestamp, a unique id, who it\'s addressed to and sometimes geolocational data. Followers have much less information available on them; there is no way to directly tell when a user followed another, or whether a user used to follow another.

\subsubsection{Twitter API}
Twitter provides two main interfaces for developers wishing to harvest data from it; a Representational State Transfer (REST) API and a streaming API. REST is a stateless protocol similar in style to the ubiquitous HTTP/1.1. As a result of prior abuse and overuse The REST API is heavily rate limited. According to the documentation non authenticated calls are limited to 150 requests per hour per IP, and OAuth-authenticated requests are limited to 350 per hour per account. Each of the exposed functions have strict limits on the amount of data they can return; the `statuses/user\_timeline' will only return the 3,200 most recent tweets of a user, and no more than 200 per request. This places a hard limit of 200x350/3200 ~ 20 user's tweets per hour. 

The initial plan was to use the streaming API since it has higher throughput more generous limits and is thus more suited to the quantity of data we wanted to obtain. However, after significant delays and repeated failures to get added to the whitelist we resorted to using the REST API with OAuth for the increased limit.

As a result of the rate limits it will need to be run for time in excess of several days to obtain a substantial amount of data, making it unsuitable for use on our desktop computers. We needed a stand alone script that could be deployed anywhere with minimal setup. SQLite was selected as a lightweight database solution that was easily setup and moved.

\subsubsection{Target Identification Algorithm}
We considered two approaches to identifying members of the target groups; manually identifying a central node and mapping outward from there, or mapping all users who use associated hashtags such as \#UAF. However, preliminary research revealed that many tags have multiple meanings; UAF is commonly used to mean `ugly as f***'. Between tag overloading and news agencies using the \#EDL tag on stories about the english defence league, tags would not have been a substantive foundation for the basic mapping algorithm. Instead, we let the operator specify a central node (easily found via Google's graph-based ranking algorithm) and the algorithm performs a breadth-first-search from there:

\begin{verbatim}
queue.add(startingUser)
while(True)
         target = queue.pop()
         for each user follower/following of target
                 queue.add(user)
         write target\'s tweets/followers/following to db
         write program state to db
\end{verbatim}

Given the low rate at which we could accumulate data, we decided to store all of it to allow for further, unforeseen uses. Simple scripts could then be used to extrapolate graphs in graphml format for our own use, and in MySQL suitable for use in last year\'s visualiser.

\subsection{Stanford Large Network Dataset Collection}
Stanford University host a project called \emph{Stanford Network Analysis Platform}, SNAP.

\begin{quote}
``SNAP is a general purpose network analysis and graph mining library. It is written in C++ and easily scales to massive networks with hundreds of millions of nodes, and billions of edges. It efficiently manipulates large graphs, calculates structural properties, generates regular and random graphs, and supports attributes on nodes and edges.''\footnote{\url{http://snap.stanford.edu}}
\end{quote}

In addition to this library, the SNAP project also hosts a collection of large datasets in the \emph{Stanford Large Network Dataset Collection}\footnote{\url{http://snap.stanford.edu/data}}. This collection of datasets includes a variety of different networks, examples including social networks, communication networks, citation networks, collaboration networks and web graphs.

The majority of these datasets are of the form $NodeId \rightarrow NodeId$, which indicates that these networks cannot be analysed from a temporal perspective. However, the sheer size of some of these networks, notably a LiveJournal network containing 4,847,571 nodes and 68,993,773 edges, and a network of Twitter users from 2009 \cite{kwak10} containing 41.7 million nodes and 1.47 billion edges, should prove DSNAT's viability in performing analysis on large-scale social networks.