\section{Influence Propogation}

\subsection{Independent Cascade}

The Independent Cascade model was implemented as two tasks; one to handle the ``dice rolling'' phase, and one to calculate the connected components of the resulting graph. The second task would be performed iteratively.

\subsubsection{Dice Rolling}

The input to this task was a file representing a graph in trivial graph format (TGF). This is a straightforward format that represents one node or edge per line; a representation which allowed the TextInputFormat and TextOutputFormat classes to be utilised for reading and writing the data. These classes allow Hadoop to easily handle text files, where each key-value pair represents one line of text (the key representing the position in the file, and the value representing the actual text).

Example TGF graph:

\begin{verbatim}
1 A false
2 B false
3 C false
4 D true
5 E false
1 2 0.19620324034685735
3 5 0.19051880402243856
3 4 0.12860629863762937
1 2 0.1707108546245374
4 5 0.12539404190086925
2 4 0.18033526297221486
1 5 0.18269135483394416
1 3 0.07440361004495522
2 5 0.13402102761559026
1 4 0.15267731629694137
\end{verbatim}

The first five lines represent the nodes of the graph, in the format: [node id] [node name] [converted at start?]. This graph has 5 nodes, and only node D is set to be converted at the start. The next ten lines represent edges, in the format [node 1] [node 2] [edge weight].

The dice rolling step was then straightforward to implement as a single Hadoop task. All the work was implemented in the map step; the basic algorithm looked like this:

\begin{verbatim}
for each key, value {
  # key is a line number and is discarded, we only care about the value

  values = parse(value)
  if values represent a node {
    if node is already influenced {
      write(nodeId, 0) # record this node is preinfluenced
    } else {
      # do nothing - we don't need to record this node
    }
  } else if values represent an edge{
    rand = rand(0,1)
    if rand < values.edgeWeight {
      write(node1id, node2id) # this edge is ``live'' and is recorded
      write(node2id, node1id)
    } else {
      # this edge is not live, so we don't need to record it.
    }
  }
}
\end{verbatim}

The reduce step merely implements the identity function and copies over the data from the map step.

The output of this task will then look something like:

\begin{verbatim}
4 0
1 2
2 1
3 4
4 3
4 5
5 4
\end{verbatim}

This might look like a strange way of representing a graph, but it was the most straightforward way to represent a graph using only key/value pairs of integers, and in a way that allows Hadoop to run a connected components algorithm on it.

Lines that end in zero (or a negative number) represent converted nodes. The line ``4 0'' shows that node 4 has been converted. Note that we do not need to record unconverted nodes -- either they are implicitly recorded in the edges or they are completely disconnected from the spanning subgraph created after the dice are rolled, and are irrelevant.

Edges are recorded in the form:

\begin{verbatim}
[node1id] [node2id]
[node2id node1id]
\end{verbatim}

Note that edges are recorded in both directions. The reason for this is explained in the next section.

\subsubsection{Connected Components}

This step was responsible for taking the spanning subgraph output by the last step and determining which nodes were connected to pre-converted nodes. This is equivalent to the standard connected components problem.

Our implementation for the connected components algorithm was based on a breadth first search. Every node that was connected to a converted node would be marked as converted. This process would then be repeated until all edges had been traversed or were unreachable.

Of course, such an algorithm could not be implemented as a single Hadoop job, and so this task had to be performed iteratively, with the output of one task being fed into the input for the next task. This process would be repeated until the termination condition was met.

This meant that each Hadoop task would represent one iteration of the algorithm; that is, each task would convert all the nodes that were direct neighbours of already converted nodes. The implementation of this was as follows:

The map step merely parsed each line of the file, each containing two integers, and split each line into a key-value pair.

The reduce step performed most of the work. It took advantage of the Hadoop framework, which passes to each reduce task a collection of all the values corresponding to a particular key. Since in our representation keys were nodeIds, that meant that each node would be processed by a different task, and each task would have all the relevant information for it's corresponding node.

This information consisted of:
\begin{itemize}
\item whether the node was already converted
\item all the edges incident on the node (this was the reason that edges were recorded in both directions).
\end{itemize}

For example, node number 4 in the above graph corresponded to the following key-value pairs:

\begin{verbatim}
4 0
4 3
4 5
\end{verbatim}

This represents the fact that node 4 had been converted, and that 3 and 5 were neighbouring nodes.

The actual algorithm worked as follows:

\begin{verbatim}
Input(key:integer, values:list(integers)
node1converted? = false
for each value in values {
  node1 = key
  node2 = value

  if node2 <= 0 {
    node1isConverted = true
    convertingNeighbour = node2
  } else {
    neighbours.add(node2)
  }
}

if node1isconverted {
	# copy this data from the last iteration
  write(node1, convertingNeighbour) 
  for neighbour in neighbours {
    write(neighbour, -node1)
    changedNodes++
  } 
} else {
    # node1 is not converted, so nothing changes, 
    # so just copy the previous values
    for neighbour in neighbours {
      write(node1, neighbours)
    }            
}
\end{verbatim}

(neighbour, -node1) is recorded for each converted neighbour. The presence of [nodeId] [negativeNumber] indicates that particular node has been converted, and inverting the negative number tells us which node converted it.

changedNodes is a counter, a global variable shared across Hadoop nodes. This counter is used to check for the termination condition; if changedNodes remains zero after an iteration, then execution halts.

A Ruby script was written to run and coordinate the different Hadoop tasks.

\subsection{Linear Threshold}

Once again, the trickiest part of the algorithm proved to be representing the state of the algorithm in a concise form that could handle both edges and nodes. The data format worked as follows:

Nodes were represented by [nodeId] [threshold]. If the threshold was equal to -1, this represented a node that had already been converted (and so we no longger cared what the threshold value was).

Edges are represented by [node1id] [node2id] [node2converted?]. Once again, edges were recorded in both directions, to ensure that each node would have all the neccesary information at the reduce step.

The algorithm then worked as follows:

The map step simply parsed the input file, setting the key to be the first integer on each line and the value to be the rest. As the value was therefore a Text object, further parsing had to be done in the reduce step. Checking whether this had a noticable effect on performance could be a future avenue for exploration.

The reduce step worked as follows:

\begin{verbatim}
Input: key:integer values:list(text)

node1isOn = false
influence = 0

for value in values {
  data = parse(value)
  if data represents node {
    if data[0] == -1 {
      node1isOn = true
    }
  } else if data represents edge {
    neighbours.add(data[0])
    if data[1] == 1 {
      influence++ # count converted neighbours
    }
  }
}

if node1isOn && influence >= threshold {
  changedNodes++
  write(key, -1) # mark this node as converted
  for neighbour in neighbours {
    write(neighbour, ``key 1'') # mark each neighbours edge so the neighbour's influence will increase on the next reduce step
  	# mark each neighbours edge so the 
  	# neighbour's influence will increase 
  	# on the next reduce step
    write(neighbour, ``key 1'') 
  }
} else {
  # nothing happens, so just copy over previous values
}
\end{verbatim}

\subsection{Graph Generation}

As per the paper [CITE], influence can spread more easily in scale-free networks. Therefore we decided to test the influence propogation algorithms by running them on several scale-free networks with different scaling factors.

A scale-free network is one where the degree distribution of nodes follows a power law, ie, the fraction P(n) of nodes in the network having n connections to other nodes is:

\(P(n) \approx ck^{-y}\)

Where c is a normalisation constant and y is a parameter with a range typically between 2 and 3.

A Ruby script was written to generate such scale free graphs. We begin with a script that would generate random graphs, using the following snippet of code:

\begin{verbatim}

def generate(num_nodes, links_per_node, num_influenced, filename)

  nodes.times do |i|
    puts i
    Node.new
  end

  (nodes * links_per_node / 2).times do |i|
    Node.add_edge
  end

end
\end{verbatim}

Where the add\_edge function looked as follows:

\begin{verbatim}
  def self.add_edge()
    while true
      node1, node2 = @@nodes.sample, @@nodes.sample
      n1,n2 = node1.node_id, node2.node_id

      next if n1 == n2
      edge = if n1 < n2
               [n1,n2]
             else
               [n2,n1]
             end

      weight = rand * 0.2
      edge.push weight

      if @@edges.any? {|n1,n2,weight| n1 == edge[0] && n2 == edge[1]}
        next
      else
        @@edges.push(edge)
        node1.neighbours += 1
        node2.neighbours += 1
        break
      end
    end
  end

\end{verbatim}

The function selects two random nodes. If the nodes are already linked, or are actually the same node, then the function tries again until they are not. When a suitable pair of nodes is found, the corresponding edge is added to the list of edges.

To generate scale free graphs, we needed to alter this code so that each node would have a maximum number of neighbours, depending on its position. This value is given by the power\_law function, which takes the node's position, and two additional parameters which correspond to c and y in the above formula. As these parameters are passed to the script on startup, graphs can be generated with varying parameters.


\begin{verbatim}
def generate(nodes, max_edges_per_node, scale_factor, num_influenced)

  nodes.times do |i|
    Node.new
  end

  Node.nodes.each_with_index do |node,i|
    max_neighbours = power_law(i,max_edges_per_node,scale_factor)

    begin
      while node.neighbours < max_neighbours
        Node.add_edge(node, max_edges_per_node, scale_factor)
      end
    rescue Exception => e
      break
    end
  end

  num_influenced.times {Node.nodes.sample.influenced = true}

  write_graph
end

def self.add_edge(node1, max_edges_per_node, scale_factor)
  100.times do
    node2 = @@unlinked_nodes.sample
    n1,n2 = node1.node_id, node2.node_id
     next if n1 == n2
    if node2.neighbours >= power_law(n2,max_edges_per_node,scale_factor)
      @@unlinked_nodes.delete_if {|n| n.node_id == n2}
      next
    end
     edge = if n1 < n2
             [n1,n2]
           else
             [n2,n1]
           end
     weight = rand * 0.2
    edge.push weight
     if node1.neighbours_array[n2]
      next
    else
      @@edges.push(edge)
      node1.neighbours += 1
      node1.neighbours_array[n2]=true
      node2.neighbours += 1
      node2.neighbours_array[n1]=true
       return
    end
  end
   throw "Graph full!"
end

\end{verbatim}

The main difference is that if adding an edge would take a node beyond its maximum number of neighbours, that edge is not added, and a new edge is found. When it becomes too difficult to find valid edges -- ie, when most nodes have reached their allocation of neighbours -- then the algorithm terminates. This means that some nodes will have less than the maximum allocation of neighbours, though in practice such nodes are very few.

In addition, the previous implementation searched the entire list of edges for a duplicate before adding a new edge. This implementation mantains a list of neighbours for each node and checks that instead, causing a great increase in performance.

To test the algorithm, 7 graphs were generated, each containing 10,000 nodes and 50,000 edges. The only difference between the graphs was the skew of their distributions; ie, in the first graph, every node had exactly 10 edges; in the last graph, the first 500 nodes had an average of 143 neigbours each, the next 1000 nodes had an average of 19 neighbours each, and the last 8500 nodes had only one edge each. The other graphs all fell somewhere between these extremes.
