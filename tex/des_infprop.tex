\subsection{Influence Propagation}

The influence propagation algorithm studied were conceptually simple. However, implementing them in a distributed computing environment like Hadoop proved complex. One major reason for this was that graph algorithms are inherently hard to parallelize. 

Hadoop works best for ``embarrasingly parallel'' problems, that is, tasks which can be broken up into subtasks where there is little dependencies between individual tasks. Graph problems such as influence propogation are inherently about dependencies, as graphs express the relationships between different pieces of data. At some point any graph processing algorithm run in a distributed environment must parition the graph into portions to be run on each indiviual node, and then decide how to communicate between these nodes without causing too slow a bottleneck.

The Pregel framework is one solution to the problem of running graph algorithms on top of Hadoop. As we had used already Pregel for some graph processing problems, we decided to implement the influence propagation algorithms in vanilla Hadoop, to explore the difficulty of implementing graph algorithms without a specialised framework.

\subsubsection{Independent Cascade}

The first step towards implementing the independent cascade algorithm on Hadoop was realising that it was best implemented as multiple tasks, rather than a single task.

Specifically, a simple way of implementing independent cascade is to first ``roll all the dice'' -- that is, decide if influence would successfully pass between each pair of connected nodes -- and then run a standard connected components algorithm.

Therefore, the algorithm was split into two tasks, one to handle the ``dice rolling'', one to handle the connected components. The first task would be run once, whereas the second task would be run iteratively, with the output of one step being fed into the next step. More details can be found below.

\paragraph{Dice Rolling}

The input to this task was a file representing a graph in trivial graph format (TGF). This is a straightforward format that represents one node or edge per line; a representation which allowed the TextInputFormat and TextOutputFormat classes to be utilised for reading and writing the data. These classes allow Hadoop to easily handle text files, where each key-value pair represents one line of text (the key representing the position in the file, and the value representing the actual text).

Example TGF graph:

\begin{verbatim}
1 A false
2 B false
3 C false
4 D true
5 E false
1 2 0.19620324034685735
3 5 0.19051880402243856
3 4 0.12860629863762937
1 2 0.1707108546245374
4 5 0.12539404190086925
2 4 0.18033526297221486
1 5 0.18269135483394416
1 3 0.07440361004495522
2 5 0.13402102761559026
1 4 0.15267731629694137
\end{verbatim}

The first five lines represent the nodes of the graph, in the format: [node id] [node name] [converted at start?]. This graph has 5 nodes, and only node D is set to be converted at the start. The next ten lines represent edges, in the format [node 1] [node 2] [edge weight].

The dice rolling step was then straightforward to implement as a single Hadoop task. All the work was implemented in the map step; the basic algorithm looked like this:

\begin{verbatim}
for each key, value {
  # key is a line number and can be discarded, we only care about the value

  values = parse(value)
  if values represent a node {
    if node is already influenced {
      write(nodeId, 0) # record this node is preinfluenced
    } else {
      # do nothing - we don't need to record this node
    }
  } else if values represent an edge{
    rand = rand(0,1)
    if rand < values.edgeWeight {
      write(node1id, node2id) # this edge is ``live'' and is recorded
      write(node2id, node1id)
    } else {
      # this edge is not live, so we don't need to record it.
    }
  }
}
\end{verbatim}

The reduce step merely implements the identity function and copies over the data from the map step.

The output of this task will then look something like:

\begin{verbatim}
4 0
1 2
2 1
3 4
4 3
4 5
5 4
\end{verbatim}

This might look like a strange way of representing a graph, but it was the most straightforward way to represent a graph using only key/value pairs of integers, and in a way that allows Hadoop to run a connected components algorithm on it.

Lines that end in zero (or a negative number) represent converted nodes. The line ``4 0'' shows that node 4 has been converted. Note that we do not need to record unconverted nodes -- either they are implicitly recorded in the edges or they are completely disconnected from the spanning subgraph created after the dice are rolled, and are irrelevant.

Edges are recorded in the form:

\begin{verbatim}
[node1id] [node2id]
[node2id node1id]
\end{verbatim}

Note that edges are recorded in both directions. The reason for this is explained in the next section.

\paragraph{Connected Components}
[http://blog.piccolboni.info/2011/04/map-reduce-algorithm-for-connected.html]

This step was responsible for taking the spanning subgraph output by the last step and determining which nodes were connected to pre-converted nodes. This is equivalent to the standard connected components problem.

Our implementation for the connected components algorithm was based on a breadth first search. Every node that was connected to a converted node would be marked as converted. This process would then be repeated until all edges had been traversed or were unreachable.

Of course, such an algorithm could not be implemented as a single Hadoop job, and so this task had to be performed iteratively, with the output of one task being fed into the input for the next task. This process would be repeated until the termination condition was met.

This meant that each Hadoop task would represent one iteration of the algorithm; that is, each task would convert all the nodes that were direct neighbours of already converted nodes. The implementation of this was as follows:

The map step merely parsed each line of the file, each containing two integers, and split each line into a key-value pair.

The reduce step performed most of the work. It took advantage of the Hadoop framework, which passes to each reduce task a collection of all the values corresponding to a particular key. Since in our representation keys were nodeIds, that meant that each node would be processed by a different task, and each task would have all the relevant information for it's corresponding node.

This information consisted of:
- whether the node was already converted
- all the edges incident on the node (this was the reason that edges were recorded in both directions).

For example, node number 4 in the above graph corresponded to the following key-value pairs:

\begin{verbatim}
4 0
4 3
4 5
\end{verbatim}

This represents the fact that node 4 had been converted, and that 3 and 5 were neighbouring nodes.

The actual algorithm worked as follows:

\begin{verbatim}
Input(key:integer, values:list(integers)

node1converted? = false
for each value in values {
  node1 = key
  node2 = value

  if node2 <= 0 {
    node1isConverted = true
    convertingNeighbour = node2
  } else {
    neighbours.add(node2)
  }
}

if node1isconverted {
  write(node1, convertingNeighbour) # copy this data from the last iteration
  for neighbour in neighbours {
    write(neighbour, -node1)
    changedNodes++
  } 
} else {
    # node1 is not converted, so nothing changes, so just copy the previous values
    for neighbour in neighbours {
      write(node1, neighbours)
    }            
}
\end{verbatim}

(neighbour, -node1) is recorded for each converted neighbour. The presence of [nodeId] [negativeNumber] indicates that particular node has been converted, and inverting the negative number tells us which node converted it.

changedNodes is a counter, a global variable shared across Hadoop nodes. This counter is used to check for the termination condition; if changedNodes remains zero after an iteration, then execution halts.

A Ruby script was written to run and coordinate the different Hadoop tasks.

\paragraph{Alternatives}

[http://blog.piccolboni.info/2010/07/map-reduce-algorithm-for-connected.html] One alternative way to implement the connected components task was using the PRAM algorithm, as described in [cite]. In theory this allows the connected components of a graph to be found in less iterations.

\subsection{Linear Threshold}

The Linear Threshold implementation follows the same basic pattern as the Independent Cascade implementation, though it only requires one task, which is performed iteratively. Each iteration corresponds to one iteration of the LT algorithm; that is, it counts all the converted neihbours of each unconverted node, and if it is greater than the node's threshold value, the node becomes converted.

Once again, the trickiest part of the algorithm proved to be representing the state of the algorithm in a concise form that could handle both edges and nodes. The data format worked as follows:

Nodes were represented by [nodeId] [threshold]. If the threshold was equal to -1, this represented a node that had already been converted (and so we no longger cared what the threshold value was).

Edges are represented by [node1id] [node2id] [node2converted?]. Once again, edges were recorded in both directions, to ensure that each node would have all the neccesary information at the reduce step.

The algorithm then worked as follows:

The map step simply parsed the input file, setting the key to be the first integer on each line and the value to be the rest. As the value was therefore a Text object, further parsing had to be done in the reduce step. Checking whether this had a noticable effect on performance could be a future avenue for exploration.

The reduce step worked as follows:

Input: key:integer values:list(text)

node1isOn = false
influence = 0

for value in values {
  data = parse(value)
  if data represents node {
    if data[0] == -1 {
      node1isOn = true
    }
  } else if data represents edge {
    neighbours.add(data[0])
    if data[1] == 1 {
      influence++ \# count converted neighbours
    }
  }
}

if node1isOn \&\& influence >= threshold {
  changedNodes++
  write(key, -1) \# mark this node as converted
  for neighbour in neighbours {
    write(neighbour, ``key 1'') \# mark each neighbours edge so the neighbour's influence will increase on the next reduce step
  }
} else {
  \# nothing happens, so just copy over previous values
}
