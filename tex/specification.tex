\chapter{Specification}

\section{Objectives}

\subsection{GUI}

The main objectives of the GUI testing and research:

In order to ensure that the GUI created by the previous years project was up to the requirements of this years project, namely two matters: the scale and adaptability of the interface. Therefore the following research and test requirements were put forward:

\begin{itemize}
\item Testing of the reverse engineered database structure through visualisation of simple graphs
\item Testing of the visualisation through increasingly larger datasets
\item  Testing of the scalability of the interface with added messages within network model
\item To analyse and develop a suitable design method to adapting the current interface to interact with SOAP messages
\item Testing of SOAP messages within current interface
\item Ensure the interface is suitable for all purposes of the other modules
\end{itemize}

The main objectives of the GUI:

\begin{itemize}
\item To communicate with the cluster via use of SOAP messages
\item To visualise data generated by the cluster
\item To upload, activate and monitor processes and actions currently being run on the cluster via use of SOAP message communication
\end{itemize}

\subsection{Twitter Data Collection}

The main objectives for this component are to construct a system which will achieve the following:

\begin{itemize}
\item Efficiently and accurately harvest data from twitter through use of the Twitter provided
 REST API ensuring that the following information is preserved for each entity:
\begin{itemize}
\item Followers
\item Following list
\item Tweets
\item Re-tweets
\item Tweet Meta-Data
\begin{itemize}
	\item Unique ID
\item Timestamp
\item Recipient Address
\item Geographical Location of Tweet (if available)
\end{itemize}
\end{itemize}
\item Harvest data in accordance to the Twitter API rules and regulations, specifically governing rate limitations of API access
\begin{itemize}
\item The harvester should access as much data from the Twitter API as permitted by the appropriate access level
\item The harvester should be able to accept data presented in the JSON format by the Twitter API
\item If necessary, the harvester should be adaptable for a higher access level should one be granted
\end{itemize}
\end{itemize}


\subsection{DSNAT}

The system objectives for DSNAT are as follows:

\begin{enumerate}
  \item Provides the ability to analyse social networks of a much larger scale than possible with SNAT
  \item Is presented via a web services interface to allow computation to occur remotely
  \item Utilises data collected for the purpose of the project, and provides the ability for users to supply their own data for analysis
  \item Provide a basis for modelling emergent behaviour on a larger scale
\end{enumerate}

The research objectives for DSNAT are for the following areas to be applied to the data we acquire to analyse networks:

\begin{enumerate}
	\item Clustering: $k$-cliques, $k$-trusses, Kernigham-Lin algorithm and Clustering coefficients
	\item Centrality: Betweenness centrality and PageRank
	\item Influence Propagation: Independent Cascade and Linear Threshold
\end{enumerate}

\subsection{Emergent Behaviours}

The objectives for the research on emergent behaviours component of the project are:

\begin{enumerate}
	\item Research on Modelling Emergent Behaviours;
	\item Evaluate research;
	\item Design an algorithm to simulate strategies of disruption;
	\item Implement the algorithm on top of the distributed social network analysis tool;
	\item Evaluate results.
\end{enumerate}

\section{Legal Issues}

We used public data from Twitter so direct issues of confidentiality and data-protection were avoided.

\subsection{Licensing}

We used the following software tools and packages during this project: Oracle Java\footnote{http://www.java.com/en/}, Apache Collections\footnote{http://commons.apache.org/collections/}, Apache Hadoop\footnote{http://hadoop.apache.org/}, Apache Giraph\footnote{http://incubator.apache.org/giraph/}, Apache Ant\footnote{http://ant.apache.org/}, Apache Maven\footnote{http://maven.apache.org/}, JUNG\footnote{http://jung.sourceforge.net/}, Perl\footnote{http://www.perl.org/}, Python\footnote{http://www.python.org/}, Cytoscape\footnote{http://www.cytoscape.org/}, and SQLite\footnote{http://sqlite.org/}. Each of these packages have permissive licences. The Apache projects are licensed under the Apache licence. JUNG is licensed under the BSD licence. In accordance with these licences, a copy of these licences is included on the attached CD. Part of the project is based on last years project which is licensed according to the LGPL which is a copy-left licence, the code produced in this project is also licensed under the LGPL. A copy of this licence is also provided on the attached CD.

\subsection{Terms Of Use}

Two stipulations of the Twitter API were relevant to us \cite{twitterTOS}:

\begin{quotation}You will not attempt or encourage others to: E) use or access the Twitter API to aggregate, 
cache (except as part of a Tweet), or store place and other geographic location information 
contained in Twitter Content.'
\end{quotation}

We don't violate this as we only store geographic information in tweets, but future extensions should be wary of it.

\begin{quotation}
`Your service should not: impersonate or facilitate impersonation of others in a manner that can mislead, confuse, or deceive users.'
\end{quotation}

Again, we don't violate this but do stray rather close to it in the intended use-case of our software.

\section{Ethical Issues}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\linewidth]{img/persona_management_flow.png}
	\caption{Persona Management}
	\label{fig:persona_management_workflow}
\end{figure}

\ref{fig:persona_management_workflow}

\subsection{Surveillance}
The primary ethical issue with this project is that it is ultimately about surveillance. As someone walking a public street having no reasonable expectation of privacy doesn't quite excuse watching their every step with CCTV cameras, a user making a post on Twitter doesn't quite justify merging it into a database in an attempt to model their relationships. This issue is exacerbated by people's tendency to view posts made on public forums as being as off-the-record as a spoken conversation with a close friend.

While it could be argued that this is merely a matter of user education, what would the result of such `education' be if not stifled, warily self-conscious communications? Who wouldn't think twice about conversing with someone known to have Interesting views, and what would the effect of such isolation be on an already borderline extremist?

\subsection{Intended Use}
The application of this information may have equally chilling consequences. A simplistic view would be to claim that ultimately this is a tool that could be used for good or bad and as such the responsibility lies with the user.  In fact, the Computer Misuse Act has something to say about `dual use articles' \cite{misuseAct} \cite{lightBlue}

`In determining the likelihood of an article being used (or misused) to commit a criminal offence, prosecutors should consider the following:
\begin{quotation}
    Has the article been developed primarily, deliberately and for the sole purpose of committing a CMA offence (i.e. unauthorised access to computer material)?
    Is the article available on a wide scale commercial basis and sold through legitimate channels?
    Is the article widely used for legitimate purposes?
    Does it have a substantial installation base?
    What was the context in which the article was used to commit the offence compared with its original intended purpose?'
\end{quotation}

So, as long as manipulating social media\cite{spyOp} is considered a legitimate purpose we're fine.

\subsection{Cracking `protected' users}
There was one boundary that we consciously decided not to cross while harvesting information from Twitter. A minority of users mark their accounts as `protected', which means that no information on their account is public. One has to send them a follow request, and then only if it is accepted will one be able to view their data. These users naturally disrupt the graph somewhat, and there are a couple of ways we could have tackled them. A fairly ethical but ineffective approach is to collate and store the information we indirectly gained on them. The alternative was to manipulate these users into accepting follow requests from our bot. Given that all our data gathering is done through a single user account, it would have been a simple matter to make the account masquerade as a member of the target group. Simply follow the most central users, and retweet the most popular tweets from those users. This would ensure that the account passed cursory examination of its `following' and tweets. This would just leave `followers' which would be easily acquired by `followbacks'; it is a common practise to follow someone who follows you, and users who follow this  practise are easily identified because they are following more people than are following them. So obtaining followers would simply be a matter of identifying these users and following them. None of this would have been a tricky addition to the existing code, but the not insignificant ethical concerns of automating impersonation on this scale prevented us.






